{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reranking Methods in RAG Systems\n",
        "\n",
        "## 개요\n",
        "재순위화는 검색 증강 생성(RAG) 시스템에서 검색된 문서의 관련성과 품질을 향상시키기 위한 중요한 단계입니다. 초기에 검색된 문서를 재평가하고 재정렬하여 후속 처리나 제시에 가장 관련성 높은 정보가 우선순위를 갖도록 합니다.\n",
        "\n",
        "## 동기\n",
        "RAG 시스템에서 재순위화를 사용하는 주요 동기는 종종 단순한 유사도 메트릭에 의존하는 초기 검색 방법의 한계를 극복하기 위함입니다. 재순위화는 더 정교한 관련성 평가를 가능하게 하며, 전통적인 검색 기법에서 놓칠 수 있는 쿼리와 문서 간의 미묘한 관계를 고려합니다. 이 과정은 생성 단계에서 가장 관련성 높은 정보가 사용되도록 보장하여 RAG 시스템의 전반적인 성능을 향상시키는 것을 목표로 합니다.\n",
        "\n",
        "## 주요 구성 요소\n",
        "재순위화 시스템은 일반적으로 다음 구성 요소를 포함합니다:\n",
        "\n",
        "1. 초기 검색기: 종종 임베딩 기반 유사도 검색을 사용하는 벡터 저장소입니다.\n",
        "2. 재순위화 모델: 다음 중 하나일 수 있습니다:\n",
        "   - 관련성 점수를 매기기 위한 대규모 언어 모델(LLM)\n",
        "   - 관련성 평가를 위해 특별히 훈련된 Cross-Encoder 모델\n",
        "3. 점수 매기기 메커니즘: 문서에 관련성 점수를 할당하는 방법\n",
        "4. 정렬 및 선택 로직: 새로운 점수를 기반으로 문서를 재정렬하기 위한 것\n",
        "\n",
        "## 방법 상세\n",
        "재순위화 과정은 일반적으로 다음 단계를 따릅니다:\n",
        "\n",
        "1. 초기 검색: 잠재적으로 관련된 문서의 초기 집합을 가져옵니다.\n",
        "2. 쌍 생성: 검색된 각 문서에 대해 쿼리-문서 쌍을 형성합니다.\n",
        "3. 점수 매기기: \n",
        "   - LLM 방법: 프롬프트를 사용하여 LLM에 문서 관련성을 평가하도록 요청합니다.\n",
        "   - Cross-Encoder 방법: 쿼리-문서 쌍을 모델에 직접 입력합니다.\n",
        "4. 점수 해석: 관련성 점수를 파싱하고 정규화합니다.\n",
        "5. 재정렬: 새로운 관련성 점수를 기반으로 문서를 정렬합니다.\n",
        "6. 선택: 재정렬된 목록에서 상위 K개 문서를 선택합니다.\n",
        "\n",
        "## 이 접근 방식의 장점\n",
        "재순위화는 여러 이점을 제공합니다:\n",
        "\n",
        "1. 관련성 향상: 더 정교한 모델을 사용함으로써 재순위화는 미묘한 관련성 요소를 포착할 수 있습니다.\n",
        "2. 유연성: 특정 요구사항과 리소스에 따라 다른 재순위화 방법을 적용할 수 있습니다.\n",
        "3. 컨텍스트 품질 향상: RAG 시스템에 더 관련성 높은 문서를 제공하면 생성된 응답의 품질이 향상됩니다.\n",
        "4. 노이즈 감소: 재순위화는 관련성이 낮은 정보를 필터링하여 가장 관련성 높은 콘텐츠에 집중하는 데 도움이 됩니다.\n",
        "\n",
        "## 결론\n",
        "재순위화는 RAG 시스템에서 검색된 정보의 품질을 크게 향상시키는 강력한 기법입니다. LLM 기반 점수 매기기 또는 전문 Cross-Encoder 모델을 사용하든, 재순위화는 문서 관련성에 대한 더 미묘하고 정확한 평가를 가능하게 합니다. 이러한 향상된 관련성은 다운스트림 작업에서 더 나은 성능으로 직접 변환되어 재순위화를 고급 RAG 구현에서 필수 구성 요소로 만듭니다.\n",
        "\n",
        "LLM 기반과 Cross-Encoder 재순위화 방법 중 선택은 필요한 정확도, 사용 가능한 계산 리소스, 특정 애플리케이션 요구사항과 같은 요소에 따라 달라집니다. 두 접근 방식 모두 기본 검색 방법에 비해 상당한 개선을 제공하며 RAG 시스템의 전반적인 효율성에 기여합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"../images/reranking-visualization.svg\" alt=\"rerank llm\" style=\"width:100%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"../images/reranking_comparison.svg\" alt=\"rerank llm\" style=\"width:100%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "아래 셀은 이 노트북을 실행하는 데 필요한 모든 패키지를 설치합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필수 패키지 설치\n",
        "#!pip install langchain langchain-openai python-dotenv sentence-transformers\n",
        "!pip install -q faiss-cpu futures python-dotenv tqdm langchain langchain-community langchain-openai langchain-text-splitters langchain-core pypdf PyMuPDF pydantic>=2.0 rank-bm25 openai tiktoken deepeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 헬퍼 함수 및 평가 모듈에 접근하기 위해 저장소 복제\n",
        "!git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
        "import sys\n",
        "sys.path.append('RAG_TECHNIQUES')\n",
        "# 최신 데이터로 실행해야 하는 경우\n",
        "# !cp -r RAG_TECHNIQUES/data ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.documents import Document  # ✅ 수정됨\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import create_retrieval_chain  # ✅ 수정됨\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain  # ✅ 추가됨\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate  # ✅ 추가됨\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field  # ✅ 추가됨 (Pydantic 모델용)\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Colab 호환성을 위해 원본 경로 추가 대체\n",
        "from helper_functions import *\n",
        "from evaluation.evalute_rag import *\n",
        "\n",
        "# .env 파일에서 환경 변수 로드\n",
        "load_dotenv()\n",
        "\n",
        "# OpenAI API 키 환경 변수 설정\n",
        "if not os.getenv('OPENAI_API_KEY'):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = input(\"Please enter your OpenAI API key: \")\n",
        "else:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 문서 경로 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필수 데이터 파일 다운로드\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# 이 노트북에서 사용되는 PDF 문서 다운로드\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"data/Understanding_Climate_Change.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 벡터 저장소 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorstore = encode_pdf(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 1: LLM based function to rerank the retrieved documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"../images/rerank_llm.svg\" alt=\"rerank llm\" style=\"width:40%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 사용자 정의 재순위화 함수 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅ Pydantic 모델을 LangChain Core의 것으로 변경\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class RatingScore(BaseModel):\n",
        "    relevance_score: float = Field(..., description=\"쿼리에 대한 문서의 관련성 점수.\")\n",
        "\n",
        "def rerank_documents(query: str, docs: List[Document], top_n: int = 3) -> List[Document]:\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"query\", \"doc\"],\n",
        "        template=\"\"\"On a scale of 1-10, rate the relevance of the following document to the query. Consider the specific context and intent of the query, not just keyword matches.\n",
        "        Query: {query}\n",
        "        Document: {doc}\n",
        "        Relevance Score:\"\"\"\n",
        "    )\n",
        "    \n",
        "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n",
        "    llm_chain = prompt_template | llm.with_structured_output(RatingScore)\n",
        "    \n",
        "    scored_docs = []\n",
        "    for doc in docs:\n",
        "        input_data = {\"query\": query, \"doc\": doc.page_content}\n",
        "        score = llm_chain.invoke(input_data).relevance_score\n",
        "        try:\n",
        "            score = float(score)\n",
        "        except ValueError:\n",
        "            score = 0  # 파싱 실패 시 기본 점수\n",
        "        scored_docs.append((doc, score))\n",
        "    \n",
        "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
        "    return [doc for doc, _ in reranked_docs[:top_n]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 문서와 관련된 샘플 쿼리로 재순위화 함수 사용 예제\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"기후 변화가 생물 다양성에 미치는 영향은 무엇인가요?\"\n",
        "initial_docs = vectorstore.similarity_search(query, k=15)\n",
        "reranked_docs = rerank_documents(query, initial_docs)\n",
        "\n",
        "# 처음 3개의 초기 문서 출력\n",
        "print(\"초기 상위 문서:\")\n",
        "for i, doc in enumerate(initial_docs[:3]):\n",
        "    print(f\"\\n문서 {i+1}:\")\n",
        "    print(doc.page_content[:200] + \"...\")  # 각 문서의 처음 200자 출력\n",
        "\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"쿼리: {query}\\n\")\n",
        "print(\"재순위화된 상위 문서:\")\n",
        "for i, doc in enumerate(reranked_docs):\n",
        "    print(f\"\\n문서 {i+1}:\")\n",
        "    print(doc.page_content[:200] + \"...\")  # 각 문서의 처음 200자 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 재순위화 기반 사용자 정의 검색기 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 사용자 정의 검색기 클래스 생성\n",
        "# ✅ LangChain 1.0 스타일로 수정\n",
        "from langchain_core.pydantic_v1 import BaseModel as PydanticBaseModel\n",
        "\n",
        "class CustomRetriever(BaseRetriever, PydanticBaseModel):\n",
        "    \n",
        "    vectorstore: Any = Field(description=\"초기 검색을 위한 벡터 저장소\")\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:\n",
        "        \"\"\"✅ get_relevant_documents 대신 _get_relevant_documents 사용\"\"\"\n",
        "        initial_docs = self.vectorstore.similarity_search(query, k=30)\n",
        "        return rerank_documents(query, initial_docs, top_n=2)\n",
        "\n",
        "    async def _aget_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:\n",
        "        \"\"\"✅ 비동기 버전 추가\"\"\"\n",
        "        raise NotImplementedError(\"비동기 검색이 구현되지 않았습니다\")\n",
        "\n",
        "\n",
        "# ✅ RetrievalQA 대신 create_retrieval_chain 사용\n",
        "\n",
        "# 사용자 정의 검색기 생성\n",
        "custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
        "\n",
        "# 질문 답변을 위한 LLM 생성\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "\n",
        "# ✅ 최신 방식: 프롬프트 템플릿 정의\n",
        "system_prompt = \"\"\"당신은 문서 기반 질의응답 AI입니다. 다음 문맥을 참고하여 정확하게 답변하세요.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "질문에 대한 답변을 작성하세요.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# ✅ Document chain 생성\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "# ✅ Retrieval chain 생성\n",
        "qa_chain = create_retrieval_chain(custom_retriever, question_answer_chain)\n",
        "\n",
        "# ✅ 기존: result = qa_chain({\"query\": query})\n",
        "# ✅ 신규: result = qa_chain.invoke({\"input\": query})\n",
        "\n",
        "query = \"기후 변화가 생물 다양성에 미치는 영향은 무엇인가요?\"\n",
        "result = qa_chain.invoke({\"input\": query})\n",
        "\n",
        "print(f\"\\n질문: {query}\")\n",
        "print(f\"답변: {result['answer']}\")  # ✅ 'result' 대신 'answer' 키 사용\n",
        "print(\"\\n관련 소스 문서:\")\n",
        "# ✅ 'source_documents' 대신 'context' 키 사용\n",
        "for i, doc in enumerate(result[\"context\"]):\n",
        "    print(f\"\\n문서 {i+1}:\")\n",
        "    print(doc.page_content[:200] + \"...\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 예제 쿼리\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = qa_chain({\"query\": query})\n",
        "\n",
        "print(f\"\\n질문: {query}\")\n",
        "print(f\"답변: {result['result']}\")\n",
        "print(\"\\n관련 소스 문서:\")\n",
        "for i, doc in enumerate(result[\"source_documents\"]):\n",
        "    print(f\"\\n문서 {i+1}:\")\n",
        "    print(doc.page_content[:200] + \"...\")  # 각 문서의 처음 200자 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 재순위화를 사용해야 하는 이유를 보여주는 예제 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparison of Retrieval Techniques\n",
            "==================================\n",
            "Query: what is the capital of france?\n",
            "\n",
            "Baseline Retrieval Result:\n",
            "\n",
            "Document 1:\n",
            "The capital of France is great.\n",
            "\n",
            "Document 2:\n",
            "The capital of France is beautiful.\n",
            "\n",
            "Advanced Retrieval Result:\n",
            "\n",
            "Document 1:\n",
            "I really enjoyed my trip to Paris, France. The city is beautiful and the food is delicious. I would love to visit again. Such a great capital city.\n",
            "\n",
            "Document 2:\n",
            "Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower. \n",
            "    I really enjoyed all the cities in france, but its capital with the Eiffel Tower is my favorite city.\n"
          ]
        }
      ],
      "source": [
        "chunks = [\n",
        "    \"The capital of France is great.\",\n",
        "    \"The capital of France is huge.\",\n",
        "    \"The capital of France is beautiful.\",\n",
        "    \"\"\"Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower. \n",
        "    I really enjoyed all the cities in france, but its capital with the Eiffel Tower is my favorite city.\"\"\", \n",
        "    \"I really enjoyed my trip to Paris, France. The city is beautiful and the food is delicious. I would love to visit again. Such a great capital city.\"\n",
        "]\n",
        "docs = [Document(page_content=sentence) for sentence in chunks]\n",
        "\n",
        "\n",
        "def compare_rag_techniques(query: str, docs: List[Document] = docs) -> None:\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "    print(\"검색 기법 비교\")\n",
        "    print(\"==================================\")\n",
        "    print(f\"쿼리: {query}\\n\")\n",
        "    \n",
        "    print(\"기준선 검색 결과:\")\n",
        "    baseline_docs = vectorstore.similarity_search(query, k=2)\n",
        "    for i, doc in enumerate(baseline_docs):\n",
        "        print(f\"\\n문서 {i+1}:\")\n",
        "        print(doc.page_content)\n",
        "\n",
        "    print(\"\\n고급 검색 결과:\")\n",
        "    custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
        "    advanced_docs = custom_retriever.get_relevant_documents(query)\n",
        "    for i, doc in enumerate(advanced_docs):\n",
        "        print(f\"\\n문서 {i+1}:\")\n",
        "        print(doc.page_content)\n",
        "\n",
        "\n",
        "query = \"프랑스의 수도는 무엇인가요?\"\n",
        "compare_rag_techniques(query, docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 2: Cross Encoder models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"../images/rerank_cross_encoder.svg\" alt=\"rerank cross encoder\" style=\"width:40%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross Encoder 클래스 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "class CrossEncoderRetriever(BaseRetriever, PydanticBaseModel):\n",
        "    vectorstore: Any = Field(description=\"초기 검색을 위한 벡터 저장소\")\n",
        "    cross_encoder: Any = Field(description=\"재순위화를 위한 Cross-encoder 모델\")\n",
        "    k: int = Field(default=5, description=\"초기에 검색할 문서 수\")\n",
        "    rerank_top_k: int = Field(default=3, description=\"재순위화 후 반환할 문서 수\")\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:\n",
        "        \"\"\"✅ 메서드명 변경: get_relevant_documents -> _get_relevant_documents\"\"\"\n",
        "        # 초기 검색\n",
        "        initial_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
        "        \n",
        "        # Cross-encoder를 위한 쌍 준비\n",
        "        pairs = [[query, doc.page_content] for doc in initial_docs]\n",
        "        \n",
        "        # Cross-encoder 점수 가져오기\n",
        "        scores = self.cross_encoder.predict(pairs)\n",
        "        \n",
        "        # 점수별로 문서 정렬\n",
        "        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        # 상위 재순위화된 문서 반환\n",
        "        return [doc for doc, _ in scored_docs[:self.rerank_top_k]]\n",
        "\n",
        "    async def _aget_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:\n",
        "        \"\"\"✅ 메서드명 변경\"\"\"\n",
        "        raise NotImplementedError(\"비동기 검색이 구현되지 않았습니다\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 인스턴스 생성 및 예제로 시연"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-encoder 검색기 생성\n",
        "cross_encoder_retriever = CrossEncoderRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    cross_encoder=cross_encoder,\n",
        "    k=10,\n",
        "    rerank_top_k=5\n",
        ")\n",
        "\n",
        "# LLM 설정\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "\n",
        "# ✅ 최신 방식으로 체인 생성\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"당신은 문서 기반 질의응답 AI입니다. 다음 문맥을 참고하여 답변하세요:\\n\\n{context}\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "qa_chain = create_retrieval_chain(cross_encoder_retriever, question_answer_chain)\n",
        "\n",
        "# ✅ 실행 방식 변경\n",
        "query = \"기후 변화가 생물 다양성에 미치는 영향은 무엇인가요?\"\n",
        "result = qa_chain.invoke({\"input\": query})\n",
        "\n",
        "print(f\"\\n질문: {query}\")\n",
        "print(f\"답변: {result['answer']}\")\n",
        "print(\"\\n관련 소스 문서:\")\n",
        "for i, doc in enumerate(result[\"context\"]):\n",
        "    print(f\"\\n문서 {i+1}:\")\n",
        "    print(doc.page_content[:200] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--reranking)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
